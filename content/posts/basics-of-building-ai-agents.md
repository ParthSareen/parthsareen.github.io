---
protected: true
---

# Basics of building AI Agents

AI agents accomplish a task in a mostly unstructured manner - where they are able to reason and conduct actions to achieve a goal. The key is in providing just enough structure through tool design, harness design, and context engineering to guide the agent effectively. 

I got interested in agents around September 2023 and started by prototyping with gpt-3.5 and gpt-4. 
We've come a long way since then, but the fundamentals remain the same. 
Approaching building these from first principles – similar to how software engineering often needs to be approached - is the best way to build agents which work well.

Last year, around January 2024, I started my own company called Extensible AI where we worked on agent reliability, frameworks, and applications. 
It was probably a bit too early to work on agent reliability but it taught me a lot about what worked and didn't work when it came to building agents. 

Don't be fooled, building agents well is still rooted in the same principles of software engineering. 
Software is often decomposed as Directed Acyclic Graphs (DAGs). 
Agents take parts of this software and make it into a black box, where the DAG is generated by the model to go from a goal to an action in a loop. 
I personally like the DAG idea quite a bit - so much so I built [DAGent](https://github.com/Extensible-AI/DAGent) last year to deal with models not being as good with agentic tool use. 

I think I've collected some interesting learnings over the last couple years and thought I'd share some of them here. 
Most recently, I've been very intrigued with agents which initiate actions without being prompted by the end user. 

I'll also focus more on open-source models as they're wildly different from the start of the year, especially with the release of Kimi-K2, Minimax M2.1, GLM-4.7, and Qwen3-coder. 
A lot of my personal workload for coding has shifted to these models. 
I think they're well suited for 90% of tasks at this point. 
Depending on your use case, some open source models might even be better picks than [closed-source models](https://artificialanalysis.ai/).

![Open vs Closed Models Performance](/images/open-vs-closed-models.png)

<center>Open vs cloud models performance from Artificial Analysis.</center>


## Anatomy of an agent

Models like `Kimi-K2`, `Minimax M2.1`, `GLM-4.7`, and `Qwen3-coder` tend to perform well in an agent loop. These models have an understanding of how to use tools they are provided and are able to go from a provided goal to conducting actions to reach that goal.

The simplest way to build an agent with capable models is to throw it into a loop with a goal and a set of tools. The following is an agent execution loop:
```python
goal = "do x for me"
message_history = [goal]
while true:
    response = model.generate(message_history, available_tools)
    message_history.append(response)
    if tool_calls:
        result = execute_tool(tool_calls)
        message_history.append(result)
    else:
        print(response)
        break
```

Under the hood, the model is in a feedback loop, much like a controls system. The model is trying to reach a set goal by conducting actions, and trying to minimize the error between the goal and the action.


## Building a simple agent

Here is a simple agent which uses a tool to search the web and then summarize the results:

```python

# Initial query
query = "who is parth sareen"
print('Query:', query)

messages = [
    {'role': 'system', 'content': 'You are a pro at making web searches. You are free to make as many searches until you satisfy [x constraints]'},
    {'role': 'user', 'content': query}
    ]
while True:
    response = chat(model='qwen3', messages=messages, tools=[web_search, web_fetch], think=True)
    if response.message.thinking:
        print('Thinking:')
        print(response.message.thinking + '\n\n')
    if response.message.content:
        print('Content:')
        print(response.message.content + '\n')
    messages.append(response.message)

    if response.message.tool_calls:
        for tool_call in response.message.tool_calls:
            function_to_call = available_tools.get(tool_call.function.name)
            if function_to_call:
                args = tool_call.function.arguments
                result = function_to_call(**args)
                print('Result from tool call:', tool_call.function.name)
                print(args)
                print(result)
                print()
                # Caps at ~2000 tokens
                messages.append({'role': 'tool', 'content': result[:2000 * 4], 'tool_name': tool_call.function.name})
            else:
                print(f'Tool {tool_call.function.name} not found')
                messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
    else:
        # No more tool calls; exit loop
        break
```

This example demonstrates the agent anatomy from earlier in practice. The agent loops until it's satisfied with the search results, then provides a final answer.

### System prompts

The system message plays a crucial role in guiding the agent's behavior and defining what it means to "satisfy a goal". Some good examples to reference are:
- [General OpenAI Codex Prompt](https://github.com/openai/codex/blob/40de81e7af928e4fa4d2e1c57df34e9522f7db65/codex-rs/core/prompt.md?plain=1#)
- [GPT-5.2-Codex Prompt](https://github.com/openai/codex/blob/40de81e7af928e4fa4d2e1c57df34e9522f7db65/codex-rs/core/gpt-5.2-codex_prompt.md?plain=1)

The prompt often needs to be tailored to different models and how they work best in order to extract the most out of them.

Depending on your use case, you may also want to provide a rubric for your agent to reason about and grade its progress, enabling better decision-making.

## Context engineering for agents
### Tool Design

I'd argue that tool design is one of the most critical and overlooked aspects of building reliable agents.
Engineers often add tools without considering what the model actually sees and reasons with when given instructions on how to solve a problem. 

Models are trained to consume tools in various different formats.
While the input to most SDKs/APIs is a JSON schema for a tool, these are usually getting transformed into a different format which the model is trained on.
This has been done through Jinja templates (or Go templates for Ollama) for the last couple years but it seems the industry is moving more towards renderers with code - see [Thinky's Tinker docs](https://tinker-docs.thinkingmachines.ai/rendering#the-renderer-class) or [Ollama's renderers](https://github.com/ollama/ollama/blob/18fdcc94e55d8ca393be9d01b30246dbbca6f6af/model/renderers/renderer.go) for examples.

Input tool schema:
```json
{
    "type": "object",
    "properties": {
        "get_location": {
            "type": "function",
            "function": {
                "name": "get_location",
                "description": "Gets the location of the user.",
                "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            }
        },
        "get_current_weather": {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Gets the current weather in the provided location.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "Temperature format",
                            "default": "celsius"
                        }
                    },
                    "required": ["location"]
                }
            }
        },
        "get_multiple_weathers": {
            "type": "function",
            "function": {
                "name": "get_multiple_weathers",
                "description": "Gets the current weather in the provided list of locations.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "locations": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            },
                            "description": "List of city and state, e.g. [\"San Francisco, CA\", \"New York, NY\"]"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "Temperature format",
                            "default": "celsius"
                        }
                    },
                    "required": ["locations"]
                }
            }
        }
    }
}

```

Rendered tool prompt:
```
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-06-28

Reasoning: high

# Valid channels: analysis, commentary, final. Channel must be included for every message.
Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions

Use a friendly tone.

# Tools

## functions

namespace functions {

// Gets the location of the user.
type get_location = () => any;

// Gets the current weather in the provided location.
type get_current_weather = (_: {
// The city and state, e.g. San Francisco, CA
location: string,
format?: "celsius" | "fahrenheit", // default: celsius
}) => any;

// Gets the current weather in the provided list of locations.
type get_multiple_weathers = (_: {
// List of city and state, e.g. ["San Francisco, CA", "New York, NY"]
locations: string[],
format?: "celsius" | "fahrenheit", // default: celsius
}) => any;

} // namespace functions<|end|><|start|>user<|message|>What is the weather like in SF?<|end|><|start|>assistant<|channel|>analysis<|message|>Need to use function get_current_weather.<|end|><|start|>assistant<|channel|>commentary to=functions.get_current_weather <|constrain|>json<|message|>{"location":"San Francisco"}<|call|><|start|>functions.get_current_weather to=assistant<|channel|>commentary<|message|>{"sunny": true, "temperature": 20}<|end|><|start|>assistant
```

<center>OpenAI Harmony format for rendered tools and tool calls.</center>

The reason why this is important is that there could be a divergence in types from the JSON schema to the rendered prompt which could lead to sub-par tool usage by the model.

I recommend not having referenced or nested types as much, and sticking to simpler types which can later be cast as needed.
It also helps to think about the pre-training and post-training of a model.
Models have probably seen less nested JSON and fewer custom types in the training corpus than non-nested JSON.

Similarly the tool output also needs to be parsed back into a JSON schema, and different providers may choose to parse in their own way depending on the inference engine they are using. Poor parsing by the provider can have two negative effects: it slowly degrades the model's tool calling capabilities, and it breaks the [KV cache](https://huggingface.co/blog/not-lain/kv-caching) since the re-rendered prompt would not match the original prompt + generated tokens. 

Certain tools you build will require their own state management and storage – think updating memory about a user, project management, long-running tasks, etc. Think about how to provide these to the model without overwhelming it with their presence. 

### Tool outputs

How you present tool results to the model significantly impacts agent performance. Consider both the format and the amount of information you return.

#### Limiting output size

In the web search example earlier, I cap results to ~2000 tokens. Search providers often return both a snippet and full page content, which can overflow the context with unnecessary information. Limiting token counts helps keep the agent focused on relevant data.

Instead of dumping all the information received from an API, clean it up and limit what you send to the model. This keeps the agent focused and efficient.

#### Understanding model expectations

Going a layer deeper to see what the model is trained on can help improve performance. Check if the model expects JSON, string, or a different format for its tool results and how the inference engine parses it.

The training of the model matters quite a bit - some models are post-trained to expect a certain format for tool results or have learned to call certain tools more effectively than others.

#### Model-specific behaviors

The following examples show how different models approach the same web search task differently:

![Example of gpt-oss using web search tools to research a task](/images/gpt-oss-searching.png)

<center>Example of gpt-oss:20b using web search tools to research a task.</center>

![Example of qwen3-coder:480b cloud using web search tools to research a task](/images/qwen3-coder-searching.png)

<center>Example of qwen3-coder:480b cloud using web search tools to research a task.</center>

It's a pretty clear difference that trained model behaviours outperform untrained/generalized behaviours. Understanding the harnesses of the model and what they're optimized for can give you an edge to build better agents.

In some cases, a smaller more optimized model may outperform a larger model if it's trained on a particular harness. 

### How to know what information to give 

Building effective agents is mostly about selecting which information to provide to the model. Just like a program execution flow, the model needs the right information at each step to successfully complete its task. 

Overall, try to provide the most relevant information to the model without filling up too much of the [context window](https://platform.claude.com/docs/en/build-with-claude/context-windows).

Depending on what kind of agent you're building, you'll need to be selective about what information to provide. 

I've generally found that providing environmental context is most effective. The agent should be aware of:
- Its overall purpose and goals
- Where it's operating
- Its limitations and capabilities
- Environmental information (date, time, current location)
- User preferences and memory
- Available tools and their purposes

For example, a research agent would need search tools plus context about the user's preferences, current date/time, and any relevant memory from past interactions.

Each model has different characteristics in how it approaches problems and uses tools. Building your own suite of benchmarks is necessary to evaluate which models work best for your specific use case.
Based on how tool use is trained for the model, what kind of tools are provided to it, and what the environment is, different models will complete the same task differently, even if the end result is the same.


### Data sources

Information is either retrieved in some manner - Database, API, Web Search, File search, etc. - or is provided to the agent in its chat history directly.

For example, if a user wants to continue planning a project, the agent needs:
- Memory of what was previously discussed
- Access to the project's current state (via tools with their own state management)

You can provide this context by either injecting it at conversation start, or by instructing the model through the system message to retrieve it from persistent storage.

### Context compression

Context compression is where the existing messages are summarized into n messages where the model can free up its context window. The summarizer you use itself would probably need to make m LLM calls to summarize the messages.  

Another strategy is to have a sliding window where the oldest messages are removed as new messages are added. This is often defined by the inference provider and sometimes tunable.

A mix of the two strategies is often used to get the best of both worlds - freshness for the recent messages and context compression for the older messages. Benchmarking your harness over long running tasks is necessary to see what works best.

## Benchmarking

With new models releasing every couple of weeks, having a way to benchmark your agent is essential.

Your harness will perform differently across models, APIs, and providers. You need to define:
- What reproducible behaviors you want to test
- Success criteria for your specific use case
- Edge cases that matter to your application

Your harness is going to perform differently for different models, APIs, and providers. 
You need to think about what reproducible behaviours you want to capture and test for in your agent. Don't overindex on public benchmarks. See how you feel about different models with your agent harness. 


## Final thoughts

It's best to build agents without too many frameworks and tools to begin with. Approach it from first principles and build out or add frameworks as needed. 

At the current rate of model progress, I expect most benchmarks to be outdated within a year.

