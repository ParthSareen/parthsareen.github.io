---
protected: true
---

# Basics of building AI Agents

AI agents accomplish a task in a mostly unstructured manner. The little structuring that is required, creates most of the value. 

I got interested in agents around September 2023 and started by prototyping with gpt-3.5 and gpt-4. 
We've come a long way since then, but the fundamentals remain the same. 
Approaching building these from first principles – similar to how software engineering often needs to be approached - is the best way to build agents which work well.

Last year, around January 2024, I started my own company called Extensible AI where we worked on agent reliability, frameworks, and applications. 
It was probably a bit too early to work on agent reliability but it taught me a lot about what worked and didn't work when it came to building agents. 

Don't be fooled, building agents well is still rooted in the same princples of software engineering. 
Software is often decomposed as Directed Acyclic Graphs (DAGs). 
Agents take parts of this software and make it into more of a black box, where the DAG is generated by the model in a sense to go from a goal to an action in a loop. 
I personally like the DAG idea quite a bit - so much so I built [DAGent](https://github.com/Extensible-AI/DAGent) last year to deal with models not being as good with agentic tool use. 

I think I've collected some interesting learnings over the last couple years and thought I'd share some of them here. 
Most recently, I've been very intrigued with agents which initiate actions without being prompted by the end user. 

I'll also focus more on open-source models as they're wildly different from the start of the year, especially with the release of Kimi-K2, Minimax M2.1, GLM-4.7, and Qwen3-coder. 
A lot of my personal workload for coding has shifted to these models. 
I think they're well suited for 90% of tasks at this point. 
Depending on your use case, some open source models might even be better picks than [closed-source models](https://artificialanalysis.ai/).

![Open vs Closed Models Performance](/images/open-vs-closed-models.png)

<center>Open vs cloud models performance from Artificial Analysis.</center>


## Anatomy of an agent

Models like `Kimi-K2`, `Minimax M2.1`, `GLM-4.7`, and `Qwen3-coder` tend to perform well an agent loop. These models have an understanding of how to use tools they are provided and are able to go from a provided goal to conducting actions to reach that goal.

The simplest way to build an agent with capable models is to throw it into a loop with a goal and a set of tools. The following is an agent execution loop:
```python
goal = "do x for me"
message_history = [goal]
while true:
    response = model.generate(message_history, available_tools)
    message_history.append(response) # Key point people often forget, critical for thinking models.
    if tool_calls:
        result = execute_tool(tool_calls)
        message_history.append(result)
    else:
        print(response)
        break
```

Under the hood, the model is in a feedback loop, much like a controls system. The model is trying to reach a set goal by conduting actions, and trying to minimize the error between the goal and the action.


## Building a simple agent

Here is a simple agent which uses a tool to search the web and then summarize the results:

```python

# Initial query
query = "who is parth sareen"
print('Query:', query)

messages = [
    {'role': 'system', 'content': 'You are a pro at making web searches. You are free to make as many searches until you satisfy [x constraints]'},
    {'role': 'user', 'content': query}
    ]
while True:
    response = chat(model='qwen3', messages=messages, tools=[web_search, web_fetch], think=True)
    if response.message.thinking:
        print('Thinking:')
        print(response.message.thinking + '\n\n')
    if response.message.content:
        print('Content:')
        print(response.message.content + '\n')
    messages.append(response.message)

    if response.message.tool_calls:
        for tool_call in response.message.tool_calls:
            function_to_call = available_tools.get(tool_call.function.name)
            if function_to_call:
                args = tool_call.function.arguments
                result = function_to_call(**args)
                print('Result from tool call:', tool_call.function.name)
                print(args)
                print(result)
                print()
                # Caps at ~2000 tokens
                messages.append({'role': 'tool', 'content': result[:2000 * 4], 'tool_name': tool_call.function.name})
            else:
                print(f'Tool {tool_call.function.name} not found')
                messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
    else:
        # No more tool calls; exit loop
        break
```

This fills in the earlier example of an agent's anatomy.
It will loop until the model is satisfied with the search and then provide a final answer.

A system message is usually provided to the model to help guide its idea of "satisfying a goal". Some good examples to reference are:
- [General OpenAI Codex Prompt](https://github.com/openai/codex/blob/40de81e7af928e4fa4d2e1c57df34e9522f7db65/codex-rs/core/prompt.md?plain=1#)
- [GPT-5.2-Codex Prompt](https://github.com/openai/codex/blob/40de81e7af928e4fa4d2e1c57df34e9522f7db65/codex-rs/core/gpt-5.2-codex_prompt.md?plain=1)

The prompt often needs to be tailored or adjusted to different models and how they like to work in order to extract the most out of them.

Depending on your use case, you may also want to provide a rubric for your agent to reason and grade its progress with so that it can make better decisions.
I am also capping the result of the web search tool to ~2000 tokens.
Often search providers give a snippet + full result of the search.
This is to prevent overflowing the context with unneccessary information. 

## Context engineering for agents
### Tool Design

I'd argue that tool design is one of the most critical and overlooked aspects of building reliable agents.
Tools are often thrown in with little thought about what the model actually sees and reasons with when given instructions on how to solve a problem. 

Models are trained to consume tools in various different formats.
While the input to most SDKs/APIs is a JSON schema for a tool, these are usually getting transformed into a different format which the model is trained on.
This has been done through Jinja templates (or Go templates for Ollama) for the last couple years but it seems the industry is moving more towards renderers with code - see [Thinky's Tinker docs](https://tinker-docs.thinkingmachines.ai/rendering#the-renderer-class) or [Ollama's renderers](https://github.com/ollama/ollama/blob/18fdcc94e55d8ca393be9d01b30246dbbca6f6af/model/renderers/renderer.go) for examples.

Input tool schema:
```json
{
    "type": "object",
    "properties": {
        "get_location": {
            "type": "function",
            "function": {
                "name": "get_location",
                "description": "Gets the location of the user.",
                "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            }
        },
        "get_current_weather": {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Gets the current weather in the provided location.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "Temperature format",
                            "default": "celsius"
                        }
                    },
                    "required": ["location"]
                }
            }
        },
        "get_multiple_weathers": {
            "type": "function",
            "function": {
                "name": "get_multiple_weathers",
                "description": "Gets the current weather in the provided list of locations.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "locations": {
                            "type": "array",
                            "items": {
                                "type": "string"
                            },
                            "description": "List of city and state, e.g. [\"San Francisco, CA\", \"New York, NY\"]"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "Temperature format",
                            "default": "celsius"
                        }
                    },
                    "required": ["locations"]
                }
            }
        }
    }
}

```

Rendered tool prompt:
```
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-06-28

Reasoning: high

# Valid channels: analysis, commentary, final. Channel must be included for every message.
Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions

Use a friendly tone.

# Tools

## functions

namespace functions {

// Gets the location of the user.
type get_location = () => any;

// Gets the current weather in the provided location.
type get_current_weather = (_: {
// The city and state, e.g. San Francisco, CA
location: string,
format?: "celsius" | "fahrenheit", // default: celsius
}) => any;

// Gets the current weather in the provided list of locations.
type get_multiple_weathers = (_: {
// List of city and state, e.g. ["San Francisco, CA", "New York, NY"]
locations: string[],
format?: "celsius" | "fahrenheit", // default: celsius
}) => any;

} // namespace functions<|end|><|start|>user<|message|>What is the weather like in SF?<|end|><|start|>assistant<|channel|>analysis<|message|>Need to use function get_current_weather.<|end|><|start|>assistant<|channel|>commentary to=functions.get_current_weather <|constrain|>json<|message|>{"location":"San Francisco"}<|call|><|start|>functions.get_current_weather to=assistant<|channel|>commentary<|message|>{"sunny": true, "temperature": 20}<|end|><|start|>assistant
```

<center>OpenAI Harmony format for rendered tools and tool calls.</center>

The reason why this is important is that there could be a divergence in types from the JSON schema to the rendered prompt which could lead to sub-par tool usage by the model.

I recommend not having referenced or nested types as much, and sticking to simpler types which can later be cast as needed.
It also helps to think about the pre-training and post-training of a model.
The amount of nested JSON and custom types a model has seen is probably less in the training corpus than non-nested JSON.

Similarly the tool output also needs to be parsed back into a JSON schema, and different providers may choose to parse in their own way depending on the inference engine they are using. If the provider does not have good parsing for model outputs, it can slowly degrade the model's tool calling capabilities, in addition to breaking the [KV cache](https://huggingface.co/blog/not-lain/kv-caching) as the re-rendered prompt would not match the original prompt + generated tokens. 

Certain tools you build will require their own state management and storage – think updating memory about a user, project management, long-running tasks, etc. Think about how to provide these to the model without overwhelming it with their presence. 

### Tool outputs

The tool result and how you present it to the model is important to consider and going a layer deeper to see what the model is trained on can help improve the agent's performance - e.g. see if the model expects JSON, string, or a different format for its tool results and how the inference engine parses it.

Instead of dumping all the information received from an API, cleaning it up, and limiting the amount of tokens sent to the model can keep the agent focused. The training of the model does matter quite a bit as some models are post-trained to expect a certain format for tool results or have learned behaviours to call certain tools more effectively than other models.


![Example of gpt-oss using web search tools to research a task](/images/gpt-oss-searching.png)

<center>Example of gpt-oss:20b using web search tools to research a task.</center>

![Example of qwen3-coder:480b cloud using web search tools to research a task](/images/qwen3-coder-searching.png)

<center>Example of qwen3-coder:480b cloud using web search tools to research a task.</center>

It's a pretty clear difference that trained model behaviours outperform untrained/generalized behaviours. Understanding the harnesses of the model and what they're oprimized for can give you an edge to build better agents.

In some cases, a smaller more optimized model may outperform a larger model if it's trained on a particular harness. 

### How to know what information to give 
Building effective agents is mostly about which information to provide to the model in order to solve a certain problem. Thinking about a program execution flow – in order for the model to complete a DAG, the correct information needs to be present for a high success rate. 

Overall, try to provide the most relevant information to the model without filling up too much of the [context window](https://platform.claude.com/docs/en/build-with-claude/context-windows).

Depending on what kind of agent you're building you'll have to withhold certain infromation and provide other information to the model. 
I've generally found that providing information from the environement is most effective. The agent should be aware of where it's operating, its limitations, and its capabilities.
From there, providing the right engineered tools is important for the model. For example, if you're building a research agent, you'll ideally want to capture the abilities the model has, as well as certain environemental information like date, time, current location, user preferences, memory, etc in addition to search tools.

Each model often has a different vibe and so building your own suite of benchmarks, tools, and honestly vibes is necessary to evaluate the agent's performance.
Based on how tool use is trained for the model, what kind of tools are provided to it, and what the environment is, different models complete the same task differently, even if the end result is the same.


### Data sources

Information is either retrieved in some manner (Database, API, Web Search, File search, etc.) or is provided to the agent in it's chat history directly.

If you're building agents which need to have information held over multiple sessions, there will be some form of persistence required and some way to either re-inject chat history if picking up a conversation or retrieve it. 

E.g. a user wants to continue working on planning for a project. The agent will need to have the correct memories and information to continue from where it left off. The tools will need to have their own state managment (project tracking in this case), and the agent needs the memory of what was discussed. This flow can either be injected at the start of a conversation or the model is instructed through the system message to retrieve the information from its persistence layer.

### Context compression

Context compression is where the existing messages are summarized into n messages where the model can free up its context window. The summarizer you use itself would probably need to make m LLM calls to summarize the messages.  

Another strategy is to have a sliding window wher the oldest messages are removed as new messages are added. This is often defined by the inference provider and sometimes tunable.

A mix of the two strategies is often used to get the best of both worlds - freshness for the recent messages and context compression for the older messages. Benchamrking your harness over long running tasks is necessary to see what works best.


## Vibes and Benchmarking

This is something I've started doing more recently, but having a way that you can benchmark the agent is necessary, as new models come almost couple of weeks at this point. 

Your harness is going to perform different for different models, APIs, and providers. 
You need to think about what reproducible behaviours you want to capture and test for in your agent. 


## Final thoughts

It's best to build agents without too many frameworks and tools to begin with. Approach it from first princples and build out or add frameworks as needed. 

At the current rate of model progrss, I expect most benchmarks to be outdated within a year.

