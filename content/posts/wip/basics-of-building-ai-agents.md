# Basics of building AI Agents

I got interested in agents around September 2023 and started by prototyping with gpt-3.5 and gpt-4. We've come a long way since then, but a lot of the core concepts remain the same.

Last year around January 2024 I started my own company called Extensible AI where we worked on agent reliability, frameworks, and applications. It was probably a bit too early to work on agent reliability but it taught me a lot about what worked and didn't work when it came to building agents. 

Don't be fooled, building agents well is still rooted in the same princples of software engineering. Software is often decomposed as Directed Acyclic Graphs (DAGs). Agents take parts of this software and make it into more of a black box, where the DAG is generated by the model in a sense to go from a goal to an action in a loop. I personally like the DAG idea quite a bit - so much so I built [DAGent](https://github.com/Extensible-AI/DAGent) last year to deal with models not being as good with *agentic tool use* (we'll cover this soon).

I think I've collected some interesting learnings over the last couple years and thought I'd share some of them here. Most recently, I've been very intrigued with agents which initiate actions without being prompted by the end user. I use poke to run simple things and I'll attempt to build a tiny version of it. 

I'll also focus more on open-source models as they've come a long way from last year, especially with the release of Kimi-K2, Minimax M2, GLM-4.6, and Qwen3-coder. A lot of my personal workload for coding has shifted to these models. I think they're well suited for 90% of tasks at this point. Depending on your use case, some open source models might even be better picks than closed-source models ![Link to mercor]

# Anatomy of an agent

The simplest way to build an agent with capable models is to throw it into a loop with a goal and a set of tools. The following is an agent execution loop:
```python
goal = "do x for me"
message_history = [goal]
while true:
    response = model.generate(message_history, available_tools)
    message_history.append(response) # Key point people often forget, critical for thinking models.
    if tool_calls:
        result = execute_tool(tool_calls)
        message_history.append(result)
    else:
        print(response)
        break
```

In the above loop, the exit condition for the model is that there are no more tools to call and it has reached a final response. You may choose to give the model the ability to end the loop itself - but I've personally never found a model to always make a good decision to "end" a loop. It often prefers a "pass" or a "continue" tool. Experimentation with your use case is best here.

Models like `Kimi-K2`, `Minimax M2`, `GLM-4.6`, and `Qwen3-coder` tend to perform well an agent loop. These models have an understanding of how to use tools they are provided and are able to go from a provided goal to conducting actions to reach that goal.


# Building a simple agent

Here is a simple agent which uses a tool to search the web and then summarize the results:

```python

# Initial query
query = "who is parth sareen"
print('Query:', query)

messages = [
    {'role': 'system', 'content': 'You are a pro at making web searches. You are free to make as many searches until you satisfy [x constraints]'},
    {'role': 'user', 'content': query}
    ]
while True:
    response = chat(model='qwen3', messages=messages, tools=[web_search, web_fetch], think=True)
    if response.message.thinking:
        print('Thinking:')
        print(response.message.thinking + '\n\n')
    if response.message.content:
        print('Content:')
        print(response.message.content + '\n')
    messages.append(response.message)

    if response.message.tool_calls:
        for tool_call in response.message.tool_calls:
            function_to_call = available_tools.get(tool_call.function.name)
            if function_to_call:
                args = tool_call.function.arguments
                result = function_to_call(**args)
                print('Result from tool call:', tool_call.function.name)
                print(args)
                print(result)
                print()
                # Caps at ~2000 tokens
                messages.append({'role': 'tool', 'content': result[:2000 * 4], 'tool_name': tool_call.function.name})
            else:
                print(f'Tool {tool_call.function.name} not found')
                messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
    else:
        # No more tool calls; exit loop
        break
```

This fills in the earlier example of an agent's anatomy. It will loop until the model is satisfied with the search and then provide a final answer. A system message can be provided to the model to help guide its idea of "satisfying a goal". Depending on your use case, you may also want to provide a rubric for your agent to reason and grade its progress with so that it can make better decisions. I am also capping the result of the web search to ~2000 tokens. Often search providers give a snippet + full result of the search. This is to prevent overflowing the context with unneccessary information. 

# Context engineering for agents
## Tool Design

I'd argue that tool design is one of the most critical and overlooked points of building reliable agents. Tools are often thrown in with little thought about what the model actually sees and reasons with when given instructions on how to solve a problem. It's a bit funny to say this to a bunch of engineers reading this, but empathizing with the model is what helps you write better tools. The more you think about what information a model is actually being given and also understanding the model itself can help you write better tools.

### Tool inputs and outputs
Models are trained to consume tools in various different formats. While the input to most SDKs/APIs is a JSON schema for a tool, these are usually getting transformed into a different format which the model is trained on. This has been done through Jinja templates (or Go templates for Ollama) for the last couple years but it seems the industry is moving more towards renderers with code [Ollama and Thinky machines].

[Example template -> rendering].

The reason why this is important is that there could be a divergence in types from the JSON schema to the rendered prompt which could lead to sub-par tool usage by the model. I recommend not having referenced or nested types as much, and sticking to simpler types which can later be cast as needed.

[Example of a simpler template]

Similarly the tool output also needs to be parsed back into a JSON schema, and different providers may choose to parse in their own way depending on the inference engine they are using. 

[Tool call being parsed back into a JSON]




## Data sources
## How to know what information to give 
## Vibes and Benchmarking
