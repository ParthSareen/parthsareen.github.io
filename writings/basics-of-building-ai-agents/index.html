<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Basics of building AI Agents | Writings</title>
  <meta name="description" content="AI agents accomplish a task in a mostly unstructured manner – where they are able to reason and conduct actions to achieve a goal. The key is in providing just enough structure through tool design, harness design, and context engineering to guide the agent effectively." />
  <link rel="icon" href="/zukohere.png" type="image/png" />
  <link rel="stylesheet" href="/style.css" />
  <script src="/theme.js"></script>
  <script src="/vim-nav.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\(', '\)']],
        displayMath: [['$$','$$'], ['\[','\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <style>
    .password-overlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: var(--bg-color, #fff);
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 9999;
    }
    .password-box {
      text-align: center;
      padding: 2rem;
    }
    .password-box input {
      padding: 0.5rem;
      font-size: 1rem;
      margin: 1rem 0;
      border: 1px solid var(--text-color, #333);
      background: var(--bg-color, #fff);
      color: var(--text-color, #333);
    }
    .password-box button {
      padding: 0.5rem 1rem;
      font-size: 1rem;
      cursor: pointer;
      background: var(--text-color, #333);
      color: var(--bg-color, #fff);
      border: none;
    }
    .error {
      color: #e74c3c;
      margin-top: 0.5rem;
    }
    .content-hidden {
      display: none;
    }
  </style>
</head>
<body>
  <div id="password-overlay" class="password-overlay">
    <div class="password-box">
      <h2>This post is password protected</h2>
      <input type="password" id="password-input" placeholder="Enter password" />
      <br />
      <button onclick="checkPassword()">Submit</button>
      <div id="error" class="error"></div>
    </div>
  </div>
  <div id="content" class="content-hidden">
    <div class="container">
      <header>
        <a href="/index.html">
          <img src="/zukohere.png" alt="Profile Image" class="profile-image" />
        </a>
        <h1><a href="/index.html">Writings</a></h1>
        <p class="post-meta"><time datetime="2025-12-25">2025-12-25</time></p>
      </header>
      <main>
        <article class="post-content">
<h1 id="basics-of-building-ai-agents" tabindex="-1">Basics of building AI Agents</h1>
<p>AI agents accomplish a task in a mostly unstructured manner – where they are able to reason and conduct actions to achieve a goal. The key is in providing just enough structure through tool design, harness design, and context engineering to guide the agent effectively.</p>
<p>I got interested in agents around September 2023 and started by prototyping with gpt-3.5 and gpt-4.
We’ve come a long way since then, but the fundamentals remain the same.
Approaching building these from first principles – similar to how software engineering often needs to be approached – is the best way to build agents which work well.</p>
<p>Last year, around January 2024, I started my own company called Extensible AI where we worked on agent reliability, frameworks, and applications.
It was probably a bit too early to work on agent reliability but it taught me a lot about what worked and didn’t work when it came to building agents.</p>
<p>Don’t be fooled, building agents well is still rooted in the same principles of software engineering.
Software is often decomposed as Directed Acyclic Graphs (DAGs).
Agents take parts of this software and make it into a black box, where the DAG is generated by the model to go from a goal to an action in a loop.
I personally like the DAG idea quite a bit – so much so I built <a href="https://github.com/Extensible-AI/DAGent">DAGent</a> last year to deal with models not being as good with agentic tool use.</p>
<p>We’re at a point where we’re going to see more and more of this DAG being generated and executed by the model. 2026 will be where we need to do less and less targeted engineering work to get the model to execute a task correctly.</p>
<p>I’ll also focus more on open-source models as they’re wildly different from the start of the year, especially with the release of Kimi-K2, Minimax M2.1, GLM-4.7, and Qwen3-coder.
A lot of my personal workload for coding has shifted to these models.
I think they’re well suited for 90% of tasks at this point.
Depending on your use case, some open source models might even be better picks than <a href="https://artificialanalysis.ai/">closed-source models</a>.</p>
<p><img src="/images/open-vs-closed-models.png" alt="Open vs Closed Models Performance"></p>
<center>Open vs cloud models performance from Artificial Analysis.</center>
<h2 id="anatomy-of-an-agent" tabindex="-1">Anatomy of an agent</h2>
<p>Models like <code>Kimi-K2</code>, <code>Minimax M2.1</code>, <code>GLM-4.7</code>, and <code>Qwen3-coder</code> tend to perform well in an agent loop. These models have an understanding of how to use tools they are provided and are able to go from a provided goal to conducting actions to reach that goal.</p>
<p>The simplest way to build an agent with capable models is to throw it into a loop with a goal and a set of tools. The following is an agent execution loop:</p>
<pre class="mermaid">graph TD
    A[Start: Goal + Message History] --> B[Model generates response]
    B --> C{Tool calls?}
    C -->|Yes| D[Execute tool]
    D --> E[Add result to history]
    E --> B
    C -->|No| F[Reached goal. Print response]
    F --> G[End]
    
    style A fill:#e2d9c9
    style G fill:#e2d9c9
    style B fill:#f6f2e8
    style C fill:#f6f2e8
    style D fill:#f6f2e8
    style E fill:#f6f2e8
    style F fill:#f6f2e8
</pre>
<pre><code class="language-python">goal = &quot;do x for me&quot;
message_history = [goal]
while true:
    response = model.generate(message_history, available_tools)
    message_history.append(response)
    if tool_calls:
        result = execute_tool(tool_calls)
        message_history.append(result)
    else:
        print(response)
        break
</code></pre>
<p>Under the hood, the model is in a feedback loop, much like a controls system. The model is trying to reach a set goal by conducting actions, and trying to minimize the error between the goal and the action.</p>
<h3 id="agents-as-control-systems-and-policies" tabindex="-1">Agents as control systems and policies</h3>
<blockquote>
<p><strong>Note:</strong> This section is optional and covers some more of the theoretical thoughts behind how agentic models get trained. Insightful but not necessary for building agents.</p>
</blockquote>
<p>From a control theory perspective, the agent is minimizing an error function at each step:</p>
<p>$$e(t) = \text{distance}(\text{goal}, \text{current state})$$</p>
<p>The model selects actions to minimize $\sum_{t} e(t)$ over the trajectory, similar to how a PID controller works to reach some steady state.</p>
<p>From a reinforcement learning perspective, this loop is executing a <strong>policy</strong> $\pi(a|s)$ where:</p>
<ul>
<li><strong>State</strong> (s): Current message history + goal</li>
<li><strong>Action</strong> (a): Tool calls or final response</li>
<li><strong>Policy</strong> (\pi): The LLM itself, generating actions given the state</li>
</ul>
<p>Thinking of these behaviours in terms of control systems and policies means that this behaviour can be trained and optimized through RL.
Modern models are increasingly post-trained using policy optimization methods like GRPO - Group Relative Policy Optimization, and PPO - Proximal Policy Optimization for agent tasks.</p>
<p>I recommend the following video by Yacine: <a href="https://www.youtube.com/watch?v=Yi1UCrAsf4o">Group Relative Policy Optimization (GRPO) - Formula and Code</a></p>
<p>The training process typically involves:</p>
<ol>
<li>Running the agent through many task trajectories</li>
<li>Comparing outcomes using reward signals - task completion, efficiency, tool usage quality</li>
<li>Using the relative performance to fine-tune the policy</li>
</ol>
<p><strong>This is where the harness becomes critical.</strong> The harness - the specific tools, system prompts, and execution environment used during training shapes how the model behaves as an agent given an environment to take actions in.
A model trained on a Codex-style harness will naturally exhibit different tool-calling patterns than one trained on an Claude code style of harness.</p>
<p>As models get better, we’re seeing more targeted post-training for specific harnesses. GPT-5.2-Codex is optimized for the Cursor/Codex CLI environment. Claude is optimized for MCP and their code environment. The harness isn’t just the runtime environment anymore - it’s a core part of the model’s learned behavior.</p>
<p>This means when you’re building agents, you’re not just writing code - you’re designing the environment that models may eventually be trained on. The better your harness design, the more likely it is that future models will work well with it out of the box.</p>
<h2 id="building-a-simple-agent" tabindex="-1">Building a simple agent</h2>
<p>Here is a simple agent which uses a tool to search the web and then summarize the results:</p>
<pre><code class="language-python"># Define the goal
query = &quot;who is parth sareen&quot;
print('Query:', query)

# Initialize message history with system prompt and user goal
# System prompt defines agent behavior and success criteria
messages = [
    {'role': 'system', 'content': 'You are a pro at making web searches. You are free to make as many searches until you satisfy [x constraints]'},
    {'role': 'user', 'content': query}
    ]

# Main agent loop
while True:
    # Model generates response based on message history and available tools
    response = chat(model='qwen3', messages=messages, tools=[web_search, web_fetch], think=True)
    
    # Some models support thinking/reasoning before acting
    if response.message.thinking:
        print('Thinking:')
        print(response.message.thinking + '\n\n')
    
    if response.message.content:
        print('Content:')
        print(response.message.content + '\n')
    
    # Add model's response to message history for context
    messages.append(response.message)

    # Check if model wants to use tools
    if response.message.tool_calls:
        for tool_call in response.message.tool_calls:
            function_to_call = available_tools.get(tool_call.function.name)
            if function_to_call:
                # Execute the tool with model's arguments
                args = tool_call.function.arguments
                result = function_to_call(**args)
                print('Result from tool call:', tool_call.function.name)
                print(args)
                print(result)
                print()
                
                # Add tool result to history (truncated to ~2000 tokens for efficiency)
                messages.append({'role': 'tool', 'content': result[:2000 * 4], 'tool_name': tool_call.function.name})
            else:
                print(f'Tool {tool_call.function.name} not found')
                messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
    else:
        # Model is satisfied and provided final answer without requesting tools
        break
</code></pre>
<p>This example demonstrates the agent anatomy from earlier in practice. The agent loops until it’s satisfied with the search results, then provides a final answer.</p>
<h3 id="system-prompts" tabindex="-1">System prompts</h3>
<p>The system message plays a crucial role in guiding the agent’s behavior and defining what it means to “satisfy a goal”. Some good examples to reference are:</p>
<ul>
<li><a href="https://github.com/openai/codex/blob/40de81e7af928e4fa4d2e1c57df34e9522f7db65/codex-rs/core/prompt.md?plain=1#">General OpenAI Codex Prompt</a></li>
<li><a href="https://github.com/openai/codex/blob/40de81e7af928e4fa4d2e1c57df34e9522f7db65/codex-rs/core/gpt-5.2-codex_prompt.md?plain=1">GPT-5.2-Codex Prompt</a></li>
</ul>
<p>The prompt often needs to be tailored to different models and how they work best in order to extract the most out of them.</p>
<p>Depending on your use case, you may also want to provide a rubric for your agent to reason about and grade its progress, enabling better decision-making.</p>
<h2 id="context-engineering-for-agents" tabindex="-1">Context engineering for agents</h2>
<h3 id="tool-design" tabindex="-1">Tool Design</h3>
<p>I’d argue that tool design is one of the most critical and overlooked aspects of building reliable agents.
Engineers often add tools without considering what the model actually sees and reasons with when given instructions on how to solve a problem.</p>
<p>Models are trained to consume tools in various different formats.
While the input to most SDKs/APIs is a JSON schema for a tool, these are usually getting transformed into a different format which the model is trained on.
This has been done through Jinja templates or Go templates for Ollama for the last couple years but it seems the industry is moving more towards renderers with code - see <a href="https://tinker-docs.thinkingmachines.ai/rendering#the-renderer-class">Thinky’s Tinker docs</a> or <a href="https://github.com/ollama/ollama/blob/18fdcc94e55d8ca393be9d01b30246dbbca6f6af/model/renderers/renderer.go">Ollama’s renderers</a> for examples.</p>
<p>The below is an example of tools JSON schemas being passed into gpt-oss and the rendered prompt that the model actually sees.</p>
<p>Input tool schema:</p>
<pre><code class="language-json">{
    &quot;type&quot;: &quot;object&quot;,
    &quot;properties&quot;: {
        &quot;get_location&quot;: {
            &quot;type&quot;: &quot;function&quot;,
            &quot;function&quot;: {
                &quot;name&quot;: &quot;get_location&quot;,
                &quot;description&quot;: &quot;Gets the location of the user.&quot;,
                &quot;parameters&quot;: {
                    &quot;type&quot;: &quot;object&quot;,
                    &quot;properties&quot;: {},
                    &quot;required&quot;: []
                }
            }
        },
        &quot;get_current_weather&quot;: {
            &quot;type&quot;: &quot;function&quot;,
            &quot;function&quot;: {
                &quot;name&quot;: &quot;get_current_weather&quot;,
                &quot;description&quot;: &quot;Gets the current weather in the provided location.&quot;,
                &quot;parameters&quot;: {
                    &quot;type&quot;: &quot;object&quot;,
                    &quot;properties&quot;: {
                        &quot;location&quot;: {
                            &quot;type&quot;: &quot;string&quot;,
                            &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;
                        },
                        &quot;format&quot;: {
                            &quot;type&quot;: &quot;string&quot;,
                            &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                            &quot;description&quot;: &quot;Temperature format&quot;,
                            &quot;default&quot;: &quot;celsius&quot;
                        }
                    },
                    &quot;required&quot;: [&quot;location&quot;]
                }
            }
        },
        &quot;get_multiple_weathers&quot;: {
            &quot;type&quot;: &quot;function&quot;,
            &quot;function&quot;: {
                &quot;name&quot;: &quot;get_multiple_weathers&quot;,
                &quot;description&quot;: &quot;Gets the current weather in the provided list of locations.&quot;,
                &quot;parameters&quot;: {
                    &quot;type&quot;: &quot;object&quot;,
                    &quot;properties&quot;: {
                        &quot;locations&quot;: {
                            &quot;type&quot;: &quot;array&quot;,
                            &quot;items&quot;: {
                                &quot;type&quot;: &quot;string&quot;
                            },
                            &quot;description&quot;: &quot;List of city and state, e.g. [\&quot;San Francisco, CA\&quot;, \&quot;New York, NY\&quot;]&quot;
                        },
                        &quot;format&quot;: {
                            &quot;type&quot;: &quot;string&quot;,
                            &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                            &quot;description&quot;: &quot;Temperature format&quot;,
                            &quot;default&quot;: &quot;celsius&quot;
                        }
                    },
                    &quot;required&quot;: [&quot;locations&quot;]
                }
            }
        }
    }
}

</code></pre>
<p>Rendered tool prompt:</p>
<pre><code>&lt;|start|&gt;system&lt;|message|&gt;You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-06-28

Reasoning: high

# Valid channels: analysis, commentary, final. Channel must be included for every message.
Calls to these tools must go to the commentary channel: 'functions'.&lt;|end|&gt;&lt;|start|&gt;developer&lt;|message|&gt;# Instructions

Use a friendly tone.

# Tools

## functions

namespace functions {

// Gets the location of the user.
type get_location = () =&gt; any;

// Gets the current weather in the provided location.
type get_current_weather = (_: {
// The city and state, e.g. San Francisco, CA
location: string,
format?: &quot;celsius&quot; | &quot;fahrenheit&quot;, // default: celsius
}) =&gt; any;

// Gets the current weather in the provided list of locations.
type get_multiple_weathers = (_: {
// List of city and state, e.g. [&quot;San Francisco, CA&quot;, &quot;New York, NY&quot;]
locations: string[],
format?: &quot;celsius&quot; | &quot;fahrenheit&quot;, // default: celsius
}) =&gt; any;

} // namespace functions&lt;|end|&gt;&lt;|start|&gt;user&lt;|message|&gt;What is the weather like in SF?&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;analysis&lt;|message|&gt;Need to use function get_current_weather.&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;commentary to=functions.get_current_weather &lt;|constrain|&gt;json&lt;|message|&gt;{&quot;location&quot;:&quot;San Francisco&quot;}&lt;|call|&gt;&lt;|start|&gt;functions.get_current_weather to=assistant&lt;|channel|&gt;commentary&lt;|message|&gt;{&quot;sunny&quot;: true, &quot;temperature&quot;: 20}&lt;|end|&gt;&lt;|start|&gt;assistant
</code></pre>
<center>OpenAI Harmony format for rendered tools and tool calls.</center>
<p>In this case, the model is trained to see the tools in a typescript-esque format and outputs tools in its own <a href="https://cookbook.openai.com/articles/openai-harmony">Harmony</a> format.</p>
<p>The reason why this is important is that there could be a divergence in types from the JSON schema to the rendered prompt which could lead to sub-par tool usage by the model.</p>
<p>I recommend not having referenced or nested types as much, and sticking to simpler types which can later be cast as needed.
It also helps to think about the pre-training and post-training of a model.
Models have probably seen less nested JSON and fewer custom types in the training corpus than non-nested JSON. This also gets more complicated when there are custom formats that a model was trained on. The more layers of abstraction we add, the higher the likelihood that something is not getting rendered correctly. I’ve spent way too much time debugging whitespaces in a rendered prompt, trust me, you want to keep tool schemas simple.</p>
<p>Similarly the tool output also needs to be parsed back into a JSON schema, and different providers may choose to parse in their own way depending on the inference engine they are using. Poor parsing by the provider can have two negative effects: it slowly degrades the model’s tool calling capabilities, and it breaks the <a href="https://huggingface.co/blog/not-lain/kv-caching">KV cache</a> since the re-rendered prompt would not match the original prompt + generated tokens.</p>
<p>Certain tools you build will require their own state management and storage – think updating memory about a user, project management, long-running tasks, etc. Providing these tools to the model while keeping their interface to the model simple is crucial to not overwhelm it.</p>
<h3 id="tool-outputs" tabindex="-1">Tool outputs</h3>
<p>How you present tool results to the model significantly impacts agent performance. Consider both the format and the amount of information you return.</p>
<h4 id="limiting-output-size" tabindex="-1">Limiting output size</h4>
<p>In the web search example earlier, I cap results to ~2000 tokens. Search providers often return both a snippet and full page content, which can overflow the context with unnecessary information. Limiting token counts helps keep the agent focused on relevant data.</p>
<p>Instead of dumping all the information received from an API, clean it up and limit what you send to the model. This keeps the agent focused and efficient.</p>
<h4 id="understanding-model-expectations" tabindex="-1">Understanding model expectations</h4>
<p>Going a layer deeper to see what the model is trained on can help improve performance. Check if the model expects JSON, string, or a different format for its tool results and how the inference engine parses it.</p>
<p>The training of the model matters quite a bit - some models are post-trained to expect a certain format for tool results or have learned to call certain tools more effectively than others.</p>
<h4 id="model-specific-behaviors" tabindex="-1">Model-specific behaviors</h4>
<p>The following examples show how different models approach the same web search task differently:</p>
<p><img src="/images/gpt-oss-searching.png" alt="Example of gpt-oss using web search tools to research a task"></p>
<center>Example of gpt-oss:20b using web search tools to research a task.</center>
<p><img src="/images/qwen3-coder-searching.png" alt="Example of qwen3-coder:480b cloud using web search tools to research a task"></p>
<center>Example of qwen3-coder:480b cloud using web search tools to research a task.</center>
<p>In the above examples, gpt-oss:20b is able to perform multiple searches, fetch content, and even search within a page for terms due to the tools it was trained on. I’d consider this being part of the agent harness of the model.</p>
<p>On the other hand, qwen3-coder:480b – which is a much bigger model, only makes a cursory search and pre-emptively returns the result due to being satisfied with the result.
There’s always going to be tradeoffs on training for specific tools and harnesses vs being a general purpose model.</p>
<p>It’s a pretty clear difference that trained model behaviours outperform untrained or generalized behaviours.
Understanding the harnesses of the model and what they’re optimized for can give you an edge to build better agents.</p>
<h3 id="how-to-know-what-information-to-give" tabindex="-1">How to know what information to give</h3>
<p>Building effective agents is mostly about selecting which information to provide to the model. Just like a program execution flow, the model needs the right information at each step to successfully complete its task.</p>
<p>Overall, try to provide the most relevant information to the model without filling up too much of the <a href="https://platform.claude.com/docs/en/build-with-claude/context-windows">context window</a>.
Depending on what kind of agent you’re building, you’ll need to be selective about what information to provide.</p>
<p>I’ve generally found that providing environmental context is most effective. The agent should be aware of:</p>
<ul>
<li>Its overall purpose and goals</li>
<li>Where it’s operating</li>
<li>Its limitations and capabilities</li>
<li>Environmental information - date, time, current location</li>
<li>User preferences and memory</li>
<li>Available tools and their purposes</li>
</ul>
<p>For example, a research agent would need search tools plus context about the user’s preferences, current date/time, and any relevant memory from past interactions.</p>
<p>Each model has different characteristics in how it approaches problems and uses tools.
Building your own suite of benchmarks is necessary to evaluate which models work best for your specific use case.
Based on how tool use is trained for the model, what kind of tools are provided to it, and what the environment is, different models will complete the same task differently, even if the end result is the same.</p>
<h3 id="data-sources" tabindex="-1">Data sources</h3>
<p>Information is either retrieved in some manner - Database, API, Web Search, File search, etc. - or is provided to the agent in its chat history directly.</p>
<p>For example, if a user wants to continue planning a project, the agent needs:</p>
<ul>
<li>Memory of what was previously discussed</li>
<li>Access to the project’s current state (via tools with their own state management)</li>
</ul>
<p>You can provide this context by either injecting it at conversation start, or by instructing the model through the system message to retrieve it from persistent storage.</p>
<p>The important note here is to provide the right information at the right time as there will be a time vs. space tradeoff – which maps to grabbing context on the fly vs. stuffing it from the start.</p>
<h3 id="context-compression" tabindex="-1">Context compression</h3>
<p>Context compression is where the existing messages are summarized into n messages where the model can free up its context window. The summarizer you use itself would probably need to make some LLM calls to summarize the messages.</p>
<p>Another strategy is to have a sliding window where the oldest messages are removed as new messages are added. This is often defined by the inference provider and sometimes tunable.</p>
<p>A mix of the two strategies is often used to get the best of both worlds - freshness for the recent messages and context compression for the older messages. Benchmarking your harness over long running tasks is necessary to see what works best.</p>
<h2 id="benchmarking" tabindex="-1">Benchmarking</h2>
<p>With new models releasing every couple of weeks, having a way to benchmark your agent is essential.</p>
<p>Your harness will perform differently across models, APIs, and providers. You need to define:</p>
<ul>
<li>What reproducible behaviors you want to test</li>
<li>Success criteria for your specific use case</li>
<li>Edge cases that matter to your application</li>
</ul>
<p>Your harness is going to perform differently for different models, APIs, and providers.
You need to think about what reproducible behaviours you want to capture and test for in your agent. Don’t overindex on public benchmarks. See how you feel about different models with your agent harness.</p>
<h2 id="final-thoughts" tabindex="-1">Final thoughts</h2>
<p>It’s best to build agents without too many frameworks and tools to begin with. Approach it from first principles and build out or add frameworks as needed.</p>
<p>At the current rate of model progress, I expect most benchmarks to be outdated within a year.</p>
<p>Agent harnesses and post-training for those particular harnesses is something I had hypothesized around the launch of Claude code, but it’s starting to become more and more apparent. E.g. GPT-5.2-Codex focusing on tools built around Codex CLI and Cursor, and Claude Sonnet and Opus focusing on post-training with MCP and Claude code.</p>
<p>I think we’ll see a rise in specialized models for specific harnesses, as well as more generalized models which can fit anywhere but maybe not perform as well. You can also see that harnesses impact overall model output quite a bit. See <a href="https://factory.ai/news/terminal-bench">Factory AI’s Droid</a>.</p>
<p>I have some fun stuff in the works – both for Ollama and my own tools. Hope to share these soon :)</p>

        </article>
        <section class="post-navigation">
          <a href="/index.html#writings">← All writings</a>
        </section>
      </main>
      <footer>
        <a href="https://github.com/parthsareen" class="icon">GitHub</a>
        <a href="https://x.com/thanosthinking" class="icon">X</a>
        <a href="https://www.linkedin.com/in/parthsareen" class="icon">LinkedIn</a>
      </footer>
    </div>
  </div>
  <script>
    const PASSWORD = 'wip2024';
    
    function initMermaid() {
      if (typeof mermaid !== 'undefined') {
        mermaid.initialize({ 
          startOnLoad: false,
          theme: 'base',
          themeVariables: {
            primaryColor: '#e2d9c9',
            primaryTextColor: '#463a2e',
            primaryBorderColor: '#744c24',
            lineColor: '#463a2e',
            secondaryColor: '#f6f2e8',
            tertiaryColor: '#e2d9c9',
            mainBkg: '#e2d9c9',
            secondBkg: '#f6f2e8',
            tertiaryBkg: '#e2d9c9',
            nodeBorder: '#744c24',
            clusterBkg: '#f6f2e8',
            clusterBorder: '#744c24',
            defaultLinkColor: '#463a2e',
            titleColor: '#463a2e',
            edgeLabelBackground: '#f6f2e8',
            fontSize: '15px',
            fontFamily: 'Spectral, serif'
          }
        });
        mermaid.run();
      }
    }
    
    function checkPassword() {
      const input = document.getElementById('password-input').value;
      if (input === PASSWORD) {
        document.getElementById('password-overlay').style.display = 'none';
        document.getElementById('content').classList.remove('content-hidden');
        localStorage.setItem('wip-auth', 'true');
        setTimeout(initMermaid, 100);
      } else {
        document.getElementById('error').textContent = 'Incorrect password';
      }
    }
    
    if (localStorage.getItem('wip-auth') === 'true') {
      document.getElementById('password-overlay').style.display = 'none';
      document.getElementById('content').classList.remove('content-hidden');
      setTimeout(initMermaid, 100);
    }
    
    document.getElementById('password-input').addEventListener('keypress', function(e) {
      if (e.key === 'Enter') {
        checkPassword();
      }
    });
  </script>
</body>
</html>